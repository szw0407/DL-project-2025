\documentclass{article}

% 中文支持设置
\usepackage{xeCJK}
\usepackage{fontspec} % 添加字体支持
\usepackage{indentfirst} % Add indentfirst package for first paragraph indentation
\setlength{\parindent}{2em} % Set paragraph indentation to 2em

% 设置西文字体
\setmainfont{Times New Roman} % 设置西文主字体为Times New Roman（衬线体）
\setsansfont{Arial} % 设置西文无衬线字体为Arial
\setmonofont{Consolas}[Scale=0.85] % 设置等宽字体，支持Unicode字符

% 设置中文字体
\setCJKmainfont[BoldFont=SimHei]{SimSun} % 设置正文字体为宋体（衬线体），加粗用黑体
\setCJKsansfont{SimHei} % 设置无衬线字体为黑体（用于标题）
\setCJKmonofont{FangSong} % 设置等宽字体为仿宋
\newCJKfontfamily\songti{SimSun} % 设置宋体字体命令
\newCJKfontfamily\heiti[BoldFont=SimHei]{SimHei} % 设置黑体字体命令
\newCJKfontfamily\kaishu{KaiTi} % 设置楷体字体命令（用于斜体替代）
\newCJKfontfamily\fangsong{FangSong} % 设置仿宋字体命令
\newCJKfontfamily\lishu{LiShu} % 设置隶书字体命令

% 页面设置
\usepackage[a4paper,top=2cm,bottom=2cm,left=3cm,right=3cm,marginparwidth=1.75cm]{geometry}
\usepackage{algorithm}
\usepackage{algpseudocode} 
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{longtable}
\usepackage{array}
\usepackage{booktabs}
\usepackage{xcolor}
\usepackage{colortbl}
\usepackage{tikz}
\usetikzlibrary{shapes.geometric, arrows, calc, positioning, fit, backgrounds, matrix}
\usepackage[unicode]{hyperref}
\usepackage{listings}
\usepackage{float} % 添加 float 宏包
\usepackage{keywords} % 引入关键词环境定义
\usepackage{codestyle} % 引入代码样式定义

% 中文标签设置
\renewcommand{\abstractname}{摘要}
\renewcommand{\contentsname}{目录}
\renewcommand{\listfigurename}{插图目录}
\renewcommand{\listtablename}{表格目录}
\renewcommand{\refname}{参考文献}
\renewcommand{\indexname}{索引}
\renewcommand{\figurename}{图}
\renewcommand{\tablename}{表}
\renewcommand{\appendixname}{附录}

\title{深度学习报告}

\usepackage{hyperref}
\begin{document}
\maketitle

\begin{abstract}
本项目旨在介绍商业智能选址预测的深度学习方法。通过分析品牌历史门店的地理分布数据，建立能够预测品牌下一家门店最优选址的模型。项目以网格（Grid）为基本地理单元，利用品牌在城市中的历史开店网格序列及每个网格的地理属性特征，挖掘品牌扩张的空间模式和内在偏好。通过对历史数据的建模与学习，预测品牌未来最有可能选址的网格位置。模型由序列编码器、空间编码器和环境编码器组成，采用多模态融合设计。实验结果表明，该模型在选址预测任务中表现优异，能够为商业决策提供科学依据。
\end{abstract}
\keywords{选址预测, 深度学习, 网格模型, 多模态融合, 序列编码器, 空间编码器, 环境编码器}
\newpage
% 目录

\tableofcontents
\newpage
\section{引言}
在现代商业环境中，选址决策是品牌扩张和市场布局过程中至关重要的一环。一个优质的门店位置不仅能够提升品牌曝光度和客流量，还直接影响企业的经济效益和市场竞争力。传统的选址方法主要依赖于专家经验和简单的人口统计分析，存在主观性强、难以量化、适应性差等局限。随着大数据、机器学习和深度学习技术的快速发展，基于数据驱动的选址预测方法为选址预测提供更多的思路启发，其能够通过对历史数据的深入分析，挖掘出潜在的空间模式和内在偏好，从而实现更科学、精准、有效的选址决策。

本课题聚焦于“商业智能选址预测”，旨在通过分析品牌历史门店的地理分布数据，建立能够预测品牌下一家门店最优选址的模型。具体而言，课题以网格（Grid）为基本地理单元，利用品牌在城市中的历史开店网格序列及每个网格的地理属性特征，挖掘品牌扩张的空间模式和内在偏好。通过对历史数据的建模与学习，预测品牌未来最有可能选址的网格位置，为企业提供科学、量化的决策依据。

本项目的数据集包含多个品牌的历史门店分布（训练集和测试集），以及覆盖研究区域的网格地理坐标信息。提供数据已划分为训练集和测试集，网格划分保证了空间分析的精度和一致性。研究区域的经纬度范围明确，便于空间特征的提取和建模。

\section{相关工作}

项目提供了多种基线（Baseline）模型或方法，包括简单模型和深度模型。在项目文档中，对比了多个基线方案的实验结果，展示了不同方法在选址预测任务中的表现，作为模型实现方案的参考和对比。

\subsection{基线}

随机猜测（Random guess）方法，将所有未出现过的网格作为候选集，随机选取若干作为预测结果\cite{Dietterich1998Approximate}。作为最简单的基线，这种方法严格意义上并不能算是预测模型，因为它没有任何的学习过程或者学习的能力，而仅能给出一个最多也只是基本符合数据整体统计分布的结果。

一般认为，一个模型至少有一点作用，就可以将它和随机猜测进行对比，如果这个模型居然结果还不如随机猜测，那么这个模型就没有意义了，一定存在很明显的缺陷，比如严重的过拟合、特征选择不当、数据标签出现错误、数据分布出现重大偏差等\cite{Domingos2012MLthingstoknow}。

\subsection{深度学习模型方法}
\subsection{Word2vec}

Word2vec是一种将离散对象映射为连续向量空间的无监督嵌入方法，最初用于自然语言处理。其核心思想是通过上下文预测目标（skip-gram）或通过目标预测上下文（CBOW），使得具有相似上下文的对象在向量空间中距离更近\cite{word2vec}。

通常认为，在复杂任务上，Skip-gram模型通常比CBOW模型表现更好，尤其是在处理大规模数据集时。Skip-gram模型通过预测上下文网格来学习目标网格的嵌入向量，能够捕捉到更丰富的语义信息。可以朴素地理解为，skip-gram试图从很有限的上下文信息中，学习到目标相对复杂的分布规律；作为对比，CBOW则是在很复杂而丰富的上下文中试图学习一个相对简单的目标分布，这在很多需要泛化能力的任务上，可能效果较差\cite{menon_empirical_2020}。因此，下文的原理举例，以及这部分的相关工作，采用的是这种方式。

\paragraph{Skip-gram模型原理}
给定序列$S = [g_1, g_2, ..., g_T]$，skip-gram的目标是最大化目标网格$g_t$在上下文窗口$C$内预测其上下文网格的概率：
\begin{equation}
\mathcal{L} = \sum_{t=1}^{T} \sum_{-c \leq j \leq c, j \neq 0} \log P(g_{t+j} | g_t)
\end{equation}
其中，$c$为窗口大小，$P(g_{t+j} | g_t)$通常通过softmax建模：
\begin{equation}
P(g_O | g_I) = \frac{\exp(\mathbf{v}_{g_O}^\top \mathbf{v}_{g_I})}{\sum_{g=1}^{|V|} \exp(\mathbf{v}_g^\top \mathbf{v}_{g_I})}
\end{equation}
其中$\mathbf{v}_{g_I}$和$\mathbf{v}_{g_O}$分别为输入和输出网格的嵌入向量，$|V|$为网格总数。

\begin{algorithm}[H]
\caption{Word2vec Skip-gram训练流程}
\begin{algorithmic}[1]
\State \textbf{输入:} 网格序列$S = [g_1, ..., g_T]$，窗口大小$c$
\State 初始化嵌入矩阵$\mathbf{V} \in \mathbb{R}^{|V| \times d}$
\For{每个位置$t$ in $1$ to $T$}
    \For{每个$j$ in $-c$ to $c$, $j \neq 0$}
        \State 取中心网格$g_t$和上下文网格$g_{t+j}$
        \State 计算$P(g_{t+j} | g_t)$
        \State 根据损失$\mathcal{L}$反向传播，更新$\mathbf{V}$
    \EndFor
\EndFor
\State \Return 训练好的嵌入$\mathbf{V}$
\end{algorithmic}
\end{algorithm}

Word2vec嵌入可用于衡量网格间的空间相似性，常见用法为取历史网格序列的平均嵌入，与候选网格嵌入计算余弦相似度进行排序预测。

基于Word2vec的嵌入方法被用到选址任务中。这种方法，虽然最初被用于自然语言处理，但是其本质是一种时序性的序列预测方法\cite{mohan_link_2021}，因此也可以被利用在这个任务中。该方法通过skip-gram模型训练每个网格的embedding，测试时将品牌历史网格序列的embedding取平均，计算与候选网格embedding的相似度（如点积、余弦相似度等），并据此排序预测结果。

\subsubsection{LSTM+MLP}

LSTM（Long Short-Term Memory，长短期记忆网络）是一种特殊的循环神经网络（RNN），能够有效捕捉序列中的长期依赖关系，解决传统RNN在长序列训练中梯度消失或爆炸的问题\cite{hochreiter_long_1997}。

\paragraph{模型结构}
LSTM单元由输入门（input gate）、遗忘门（forget gate）、输出门（output gate）和细胞状态（cell state）组成。其结构如图\ref{fig:lstm_structure}所示。

\begin{figure}[H]
\centering
\begin{tikzpicture}[
    node distance=1.5cm,
    auto,
    >=latex',
    input/.style={circle, draw, fill=blue!20, minimum size=0.8cm, font=\small},
    gate/.style={rectangle, draw, fill=purple!20, minimum size=0.8cm, font=\small},
    state/.style={rectangle, draw, fill=green!20, minimum size=1cm, font=\small},
    operation/.style={circle, draw, fill=gray!20, minimum size=0.6cm, font=\tiny},
    arrow/.style={->, thick}
]

% 输入节点 - 放在门控单元下方
\node[input] (xt) at (2,-1.5) {$x_t$};
\node[input] (ht_1) at (4,-1.5) {$h_{t-1}$};
\node[state] (ct_1) at (-2.5,2.5) {$C_{t-1}$};

% 门控单元 - 等间距排列，文字在内部
\node[gate, minimum width=1.2cm, minimum height=0.8cm, align=center] (fg) at (1,0.5) {\small 遗忘门\\$f_t$};
\node[gate, minimum width=1.2cm, minimum height=0.8cm, align=center] (cg) at (3,0.5) {\small 候选值\\$\tilde{C}_t$};
\node[gate, minimum width=1.2cm, minimum height=0.8cm, align=center] (ig) at (5,0.5) {\small 输入门\\$i_t$};
\node[gate, minimum width=1.2cm, minimum height=0.8cm, align=center] (og) at (7,0.5) {\small 输出门\\$o_t$};

% 操作节点
\node[operation] (mult1) at (0,2.5) {$\times$};
\node[operation] (mult2) at (4,2.5) {$\times$};
\node[operation] (add) at (2,3.5) {$+$};
\node[operation] (mult3) at (7,4) {$\times$};
\node[operation] (tanh_out) at (5,4) {tanh};

% 状态节点
\node[state] (ct) at (2,4.5) {$C_t$};
\node[state] (ht) at (7,5) {$h_t$};

% 连接线
\draw[arrow] (xt) -- (fg);
\draw[arrow] (xt) -- (cg);
\draw[arrow] (xt) -- (ig);
\draw[arrow] (xt) -- (og);

\draw[arrow] (ht_1) -- (fg);
\draw[arrow] (ht_1) -- (cg);
\draw[arrow] (ht_1) -- (ig);
\draw[arrow] (ht_1) -- (og);

\draw[arrow] (ct_1) -- (mult1);
\draw[arrow] (fg) -- (mult1);
\draw[arrow] (ig) -- (mult2);
\draw[arrow] (cg) -- (mult2);

\draw[arrow] (mult1) -- (add);
\draw[arrow] (mult2) -- (add);
\draw[arrow] (add) -- (ct);

\draw[arrow] (ct) -- (tanh_out);
\draw[arrow] (og) -- (mult3);
\draw[arrow] (tanh_out) -- (mult3);
\draw[arrow] (mult3) -- (ht);

\end{tikzpicture}
\caption{LSTM单元结构图}
\label{fig:lstm_structure}
\end{figure}

LSTM的核心机制包括三个门控单元和细胞状态：

\begin{itemize}
\item \textbf{遗忘门}：决定从细胞状态中丢弃什么信息，公式为$f_t = \sigma(W_f \cdot [h_{t-1}, x_t] + b_f)$
\item \textbf{输入门}：决定什么新信息被存储在细胞状态中，公式为$i_t = \sigma(W_i \cdot [h_{t-1}, x_t] + b_i)$
\item \textbf{候选值}：创建新的候选值向量，公式为$\tilde{C}_t = \tanh(W_C \cdot [h_{t-1}, x_t] + b_C)$
\item \textbf{细胞状态更新}：结合遗忘门和输入门的结果，公式为$C_t = f_t * C_{t-1} + i_t * \tilde{C}_t$
\item \textbf{输出门}：决定细胞状态的哪些部分将输出，公式为$o_t = \sigma(W_o \cdot [h_{t-1}, x_t] + b_o)$
\item \textbf{隐藏状态}：最终的输出，公式为$h_t = o_t * \tanh(C_t)$
\end{itemize}

LSTM能够有效捕捉序列中的长期依赖信息，广泛应用于序列建模、时间序列预测、自然语言处理等任务。

深度学习方法方面，LSTM+MLP结构利用嵌入层将输入网格序列编码为向量，随后通过LSTM捕捉序列中的时序依赖\cite{hochreiter_long_1997}，最后通过多层感知机（MLP）进行分类预测。

\subsubsection{Transformer+MLP}

Transformer+MLP则用Transformer替代LSTM，利用自注意力机制更好地建模序列中各网格之间的复杂关系\cite{vaswani_attention_2023}，进一步基于注意力融合和对比学习的方法，通过对输入序列进行self-attention，获得全局语义表达，并采用InfoNCE损失进行正负样本的对比训练，有效提升了模型的判别能力和泛化性能。

\subsection{对比学习}

基于注意力融合的对比学习方法（Attention-based-fusion+对比学习）进一步提升了模型的表达能力。具体而言，首先给定品牌的历史网格序列，将其中一个网格作为ground truth，其余网格作为输入序列。对输入序列中的每个grid进行嵌入（embedding），然后通过自注意力（self-attention）机制对序列进行编码，获得每个位置的上下文表示。将self-attention后的序列embedding取平均，得到该序列的全局表示（anchor embedding）。以ground truth对应的grid embedding作为正样本，将所有未出现在历史序列中的grid中随机选取若干作为负样本，提取其embedding。最终，采用InfoNCE损失函数进行正负样本的对比训练，有效提升了模型对空间选址的判别能力和泛化性能。

\begin{algorithm}[H]
\caption{基于对比学习的选址预测训练流程}
\begin{algorithmic}[1]
\State \textbf{输入:} 品牌历史网格序列 $S = [g_1, ..., g_T]$，所有网格集合 $V$，负样本数量 $K$
\State 初始化嵌入模型 $\text{embed}(\cdot)$ 和自注意力模型 $\text{SelfAttention}(\cdot)$
\For{每个训练迭代}
    \State 从序列 $S$ 中随机选择一个网格 $g_t$ 作为正样本 $g_{pos}$
    \State 将剩余的网格 $S' = S \setminus \{g_t\}$ 作为输入序列
    \State 从 $V \setminus S$ 中随机采样 $K$ 个网格作为负样本集 $G_{neg}$
    
    \State \Comment{编码输入序列以生成锚点表示}
    \State 对输入序列 $S'$ 中的每个网格进行嵌入: $E_{S'} = [\text{embed}(g) \text{ for } g \in S']$
    \State 通过自注意力机制处理嵌入序列: $H = \text{SelfAttention}(E_{S'})$
    \State 计算锚点嵌入: $\mathbf{v}_{anchor} = \text{mean}(H)$
    
    \State \Comment{编码正负样本}
    \State 编码正样本: $\mathbf{v}_{pos} = \text{embed}(g_{pos})$
    \State 编码负样本: $\mathbf{V}_{neg} = [\text{embed}(g) \text{ for } g \in G_{neg}]$
    
    \State \Comment{计算InfoNCE损失}
    \State 计算正样本相似度: $s_{pos} = \text{sim}(\mathbf{v}_{anchor}, \mathbf{v}_{pos})$
    \State 计算负样本相似度: $s_{neg,k} = \text{sim}(\mathbf{v}_{anchor}, \mathbf{v}_{neg,k})$ for $k=1, ..., K$
    \State 计算损失 $\mathcal{L} = -\log \frac{\exp(s_{pos} / \tau)}{\exp(s_{pos} / \tau) + \sum_{k=1}^{K} \exp(s_{neg,k} / \tau)}$
    
    \State \Comment{更新模型参数}
    \State 根据损失 $\mathcal{L}$ 反向传播，更新模型参数
\EndFor
\State \Return 训练好的模型
\end{algorithmic}
\end{algorithm}

Baseline 的参考指标如表\ref{tab:baseline_results}所示。

\begin{table}[H]
\centering
\begin{tabular}{|c|c|c|c|c|}
\hline
\rowcolor[HTML]{D9EAD3}
\textbf{方法} & \textbf{acc@1} & \textbf{acc@5} & \textbf{acc@10} & \textbf{Mean rank} \\ \hline
随机猜测 & 0.0165 & 0.0620 & 0.1141 & 0.0373 \\ \hline
Word2vec & 0.0446 & 0.2625 & 0.3622 & 0.1288 \\ \hline
LSTM+MLP & 0.0414 & 0.1691 & 0.2790 & 0.0976 \\ \hline
Transformer+MLP & 0.0525 & 0.1665 & 0.2759 & 0.1027 \\ \hline
对比学习 & 0.0604 & 0.2296 & 0.3702 & 0.1326 \\ \hline
\end{tabular}
\caption{Baseline方法在选址预测任务上的实验结果}
\label{tab:baseline_results}
\end{table}


上述方法在本项目数据集上的实验结果如表所示。可以看出，深度学习模型（如Transformer+MLP、对比学习方法）在准确率和平均排序等指标上均优于传统方法，尤其是对比学习方法在acc@1和acc@10等指标上表现突出，显示了其在空间选址预测任务中的潜力。此外，模型结构的层级设计（如嵌入层、序列建模层、融合层和输出层）对最终性能有显著影响，合理的结构选择和层级组合能够更好地捕捉空间和序列特征，从而提升预测效果。

\section{方法探索}

实验的目标是优化模型结构，使之表现优于基线模型。通过对比不同的模型架构和特征处理方法，探索最优的选址预测方案。因此，考虑到数据集的特点和任务需求，我们设计了一个多模态融合的神经网络架构，该架构能够同时处理和融合三种不同类型的信息：序列特征、空间特征和环境特征。我们通过对数据进行预处理和增加有关特征的方式，添加了序列和环境特征，此外充分利用深度模型的各种特性，对其和空间特征共同编码处理，从而实现对品牌选址的更加精准的预测。

\subsection{数据预处理}

数据集中有已经分配好的训练集和测试集。数据的内容主要是品牌历史门店的选址信息。具体分析数据结构如表\ref{tab:dataset_fields}所示。

\begin{table}[H]
\centering
\begin{tabular}{|c|c|c|}
\hline
\rowcolor[HTML]{D9EAD3}
\textbf{字段} & \textbf{类型} & \textbf{描述} \\ \hline
brand\_name & string & 品牌名称 \\ \hline
brand\_type & string(csv) & 品牌类型（如餐饮、零售等） \\ \hline
longtitude\_list & list(float) & 网格经度列表 \\ \hline
latitude\_list & list(float) & 网格纬度列表 \\ \hline
grid\_id\_list & list(int) & 网格ID列表 \\ \hline
\end{tabular}
\caption{数据集字段说明}
\label{tab:dataset_fields}
\end{table}


此外还通过了网格数据，给出对应网格ID在真实地理世界中的经纬度坐标。网格数据的结构如表\ref{tab:grid_fields}所示。

\begin{table}[H]
\centering
\begin{tabular}{|c|c|c|}
\hline
\rowcolor[HTML]{D9EAD3}
\textbf{字段} & \textbf{类型} & \textbf{描述} \\ \hline
grid\_id & int & 网格ID \\ \hline
grind\_lon\_min & float & 网格左下角经度 \\ \hline
grind\_lon\_max & float & 网格右上角经度 \\ \hline
grind\_lat\_min & float & 网格左下角纬度 \\ \hline
grind\_lat\_max & float & 网格右上角纬度 \\ \hline
\end{tabular}
\caption{网格数据字段说明}
\label{tab:grid_fields}
\end{table}

\subsubsection{数据加载}

数据加载是数据预处理的第一步。我们需要从提供的训练集和测试集文件中读取品牌历史门店的选址信息，并将其转换为适合模型输入的格式，提交给神经网络。

\subsubsection{网格信息加载}

“坐标”含有的信息过于的稀少，难以直接被模型“理解”，从而掌握其中内在的一些联系。需要对数据增加有关的信息，以扩展其表达能力，否则模型表达能力的上限就会受到限制\cite{Shalev-Shwartz2014UnderstandingML}。为了得出更加合理的结论，必须在数据预处理阶段强化并且增加特征。

网格（Grid）是商业选址预测任务中的基本地理单元。在原始数据集里，每个网格由其唯一的网格ID和对应的地理坐标（经纬度）组成。这个体现了网格在空间上的位置信息。但是这个信息并不能很有效地描述网格的全部特性。为了增强网格的信息，需要从数据集中重新挖掘出可能存在的信息。兴趣点（POI, Point of Interest）是地理信息系统（GIS）和位置服务中非常核心的概念，它指的是地图上具有特定意义或吸引力的地点。它代表一个具有空间位置和语义信息的地理目标，比如餐厅、医院、公交站、商场、景点等，在经纬度以外，其还通常包括有名称、地址、类别等信息，并经常以向量的形式表示。POI 可以将三维世界中的复杂实体抽象为一个零维点，从而便于管理、分析和计算\cite{psyllidis_points_2022}。

因此，可以从全部的门店数据中，统计每个网格的相关信息，再将其关联到网格ID上，构成更加立体、具有丰富特征的高维网格对象。具体而言，再项目实现上，从网格坐标文件中提取每个网格的地理信息，包括经纬度坐标和10种类型的POI特征（医疗、住宿、摩托、体育、餐饮、公司、购物、生活、科教、汽车）。这种类型的统计信息，虽然引入了一些和其他若干特征高度相关的特征，但是会在大型复杂模型的训练上甚至有性能提升的作用\cite{Heaton_2016}。

\subsubsection{特征归一化}

为了消除不同特征间的尺度差异，我们对坐标和POI特征分别进行归一化处理：

对于坐标特征，我们计算网格的中心点坐标，然后将其归一化到[0,1]区间：
\begin{equation}
x_{norm} = \frac{x - x_{min}}{x_{max} - x_{min} + \epsilon}
\end{equation}

对于POI特征，我们同样采用归一化方法，确保不同类型POI特征的数值在同一尺度上。

\subsubsection{基于密度的序列排序}

考虑到商业扩张通常遵循"从高密度区域向外扩展"的规律，我们设计了基于密度的序列排序算法。该算法通过计算每个网格到其K个最近邻网格的平均距离来衡量密度，距离越小表示密度越大：

\begin{equation}
density\_score_i = \frac{1}{K}\sum_{j=1}^{K} d(grid_i, neighbor_j)
\end{equation}

其中$d(grid_i, neighbor_j)$表示网格$i$到其第$j$个最近邻的欧氏距离。通过这种排序方式，我们能够构建符合商业扩张规律的序列，提高模型的学习效果。

\subsubsection{滑动窗口样本生成}

为了最大化利用有限的训练数据，我们采用滑动窗口方法从每个品牌的店铺序列中生成多个训练样本。例如，对于序列[A,B,C,D]，我们使用以下算法产生样本：

\begin{algorithm}
\caption{滑动窗口样本生成算法}
\begin{algorithmic}[1]
\State \textbf{输入:} 序列 $S = [A, B, C, D]$, 窗口大小 $w = 3$
\State \textbf{输出:} 样本集 $Samples$
\State $Samples \gets []$
\For{$i = 0$ to $len(S) - w$}
    \State $sample \gets S[i:i+w]$
    \State $Samples.append(sample)$
\EndFor
\State \Return $Samples$
\end{algorithmic}
\end{algorithm}

这种处理方法不仅增加了训练样本数量，还使模型能够学习不同长度序列的预测模式，更好地模拟品牌实际扩张过程中的决策链。


\subsection{对于标签的处理尝试}

% TODO: 用户可在此补充对标签处理的具体尝试，如类别不平衡处理、标签平滑等

\subsection{数据增强和特征增强}

% TODO: 用户可在此补充数据增强的具体方法，如噪声注入、序列扰动、特征工程等

\subsection{多模态特征融合设计}

针对商业选址预测任务的特点，我们设计了一个多模态融合的神经网络架构，该架构能够同时处理和融合三种不同类型的信息：

\subsubsection{序列特征编码}
历史选址序列包含了品牌扩张的时序信息和空间模式。我们使用嵌入层将网格ID映射到低维向量空间，然后通过LSTM网络捕获序列中的时序依赖关系：

\begin{equation}
h_t = LSTM(embed(grid\_id_t), h_{t-1})
\end{equation}

LSTM的长短期记忆特性使其能够有效学习品牌在不同时期的选址偏好变化。

\subsubsection{空间特征编码}
地理坐标信息反映了选址的空间连续性。我们通过多层感知机(MLP)对归一化后的坐标特征进行编码，提取空间分布模式。

\subsubsection{环境特征编码}
POI特征反映了网格周边的商业环境和设施分布。同样使用MLP对POI特征向量进行编码，捕获环境因素对选址决策的影响。

\subsubsection{特征融合策略}
我们采用特征拼接的方式将三种编码后的特征向量组合，然后通过多层融合网络进一步提取联合特征：

\begin{equation}
f_{fused} = MLP_{fusion}(concat(f_{seq}, f_{coord}, f_{poi}))
\end{equation}

最终通过线性分类器预测下一个网格的概率分布。

\subsection{模型最终设计}

考虑到数据集中特征的特点，模型采用时序的方案进行建模。整体架构采用多模态融合设计，具体包括：

\begin{itemize}
\item \textbf{序列编码器}：基于嵌入层和LSTM的序列特征提取
\item \textbf{空间编码器}：基于MLP的坐标特征编码
\item \textbf{环境编码器}：基于MLP的POI特征编码  
\item \textbf{融合网络}：多层感知机进行特征融合
\item \textbf{分类器}：线性层输出概率分布
\end{itemize}

该设计充分考虑了商业选址的多因素特性，通过端到端的学习方式自动发现不同模态特征间的内在关联，为精准的选址预测提供了有力支撑。

% TODO: 用户可在此补充模型设计的其他尝试，如不同架构的对比、超参数调优等

\addcontentsline{toc}{section}{\refname}
\bibliographystyle{IEEEtran}
\bibliography{sample}
\newpage
\appendix
\section*{\appendixname}
\addcontentsline{toc}{section}{\appendixname}
\section{项目代码实现}

\textbf{GitHub仓库地址：} \url{https://github.com/szw0407/DL-project-2025}

\subsection{核心模型代码}

此部分的代码中注释有一部分是由AI生成的，旨在更清晰地展示思路，体现模型的结构和各个组件的功能。

主体代码实现分为多个模块，主要包括神经网络模型实现、主程序入口、训练逻辑实现、数据预处理模块、数据特征增强和模型评估模块等。

\subsubsection{神经网络模型实现 (model.py)}
\lstinputlisting[language=Python, caption={多模态神经网络模型实现}, label={lst:model}]{../src/model.py}

\subsubsection{主程序入口 (main.py)}
\lstinputlisting[language=Python, caption={主程序实现}, label={lst:main}]{../src/main.py}

\subsubsection{训练逻辑实现 (train.py)}
\lstinputlisting[language=Python, caption={模型训练实现}, label={lst:train}]{../src/train.py}

\subsubsection{数据预处理模块 (data\_preprocessing.py)}
\lstinputlisting[language=Python, caption={数据预处理实现}, label={lst:preprocessing}]{../src/data_preprocessing.py}

\subsubsection{数据特征增强 (测试数据文件.py)}
\lstinputlisting[language=Python, caption={数据特征增强实现}, label={lst:test_data}]{../src/测试数据文件.py}

\subsubsection{模型评估模块 (evaluate.py)}
\lstinputlisting[language=Python, caption={模型评估实现}, label={lst:evaluate}]{../src/evaluate.py}

\subsection{数据样本展示}
\subsubsection{训练数据格式}
以下直接展示训练数据文件的前10行内容：
\inputdatafile{../data/train_data.csv}{训练数据样本 (train\_data.csv)}{lst:train_data}

\subsubsection{网格坐标映射}
以下直接展示网格坐标映射文件的前10行内容：
\inputdatafile{../data/grid_coordinates-2.csv}{网格坐标映射数据 (grid\_coordinates-2.csv)}{lst:grid_data}

\subsubsection{测试数据格式}
以下直接展示测试数据文件的前10行内容：
\inputdatafile{../data/test_data.csv}{测试数据样本 (test\_data.csv)}{lst:test_data}

\subsection{完整源代码文件结构}

项目包含以下主要文件：

\begin{itemize}
    \item \texttt{src/model.py} - 神经网络模型定义
    \item \texttt{src/main.py} - 主程序入口
    \item \texttt{src/train.py} - 训练逻辑实现
    \item \texttt{src/evaluate.py} - 模型评估
    \item \texttt{src/data\_preprocessing.py} - 数据预处理
    \item \texttt{data/train\_data.csv} - 训练数据
    \item \texttt{data/test\_data.csv} - 测试数据
    \item \texttt{data/grid\_coordinates-2.csv} - 网格坐标映射
\end{itemize}

详细的代码实现请参考GitHub仓库：\url{https://github.com/szw0407/DL-project-2025}

\end{document}